[["index.html", "Quantitative Ecology Modelling applications in population ecology Chapter 1 Setup 1.1 Setup Python 1.2 Setup Julia", " Quantitative Ecology Modelling applications in population ecology Matt Weldy 2021-09-22 Chapter 1 Setup 1.1 Setup Python 1.2 Setup Julia Was the setup correct a = sqrt(2) ## 1.4142135623730951 "],["prerequisites.html", "Chapter 2 Prerequisites", " Chapter 2 Prerequisites This is a work in progress. The scope of this work is to demonstrate the fit of well known ecological models using a variety of tools. Our aim is to provide brief model descriptions, primary citations, and a simple simulation and model fit. Contributions to this web page are welcome, and can be made through the books github repository. The website is hosted through netlify, which offers continuous integration with the books git repository. Commit changes to the repository after using the bookdown and changes will populate to the page. Please try to structure indivudal model fits using the template outlined in Appendix B. bookdown::render_book(&quot;index.Rmd&quot;, &quot;bookdown::gitbook&quot;) "],["intro.html", "Chapter 3 Introduction 3.1 Probabilistic Modelling in Ecology 3.2 Common Distributions 3.3 Uncommon Distributions 3.4 Linear Models 3.5 Common Priors", " Chapter 3 Introduction 3.1 Probabilistic Modelling in Ecology 3.2 Common Distributions 3.3 Uncommon Distributions 3.4 Linear Models library(lme4) library(R2jags) library(runjags) #library(equatiomatic) 3.4.1 Model of the Mean This is an intercept only model. We are estimating two parameters, \\(\\mu\\) and \\(\\sigma\\). Average length of adult coho salmon 75 cm with standard deviation of 20. n &lt;- 1000 mu &lt;- 75 sd &lt;- 20 y &lt;- rnorm(n, mean = mu, sd = sd) hist(y) \\[y \\sim Normal(\\mu, \\sigma)\\] \\[mu = \\beta_0\\] \\[P(\\mu, \\sigma| y) \\propto \\mathbb(L)*P\\] lm_fit &lt;- lm(y ~ 1) data &lt;- list( n = n, y = y ) model_string &lt;- textConnection( &quot; model { # Likelihood for (i in 1:n){ y[i] ~ dnorm(mu[i], tau) residuals[i] &lt;- y[i] - mu[i] mu[i] &lt;- intercept } # Priors intercept ~ dnorm(0, 0.00001) sigma ~ dunif(0, 100) # standard deviation tau &lt;- 1 / (sigma * sigma) # sigma^2 doesn&#39;t work in JAGS # Derived values residual_sum_sq &lt;- sum(residuals[]^2) } &quot; ) parameters &lt;- c(&quot;intercept&quot;, &quot;sigma&quot;,&quot;residual_sum_sq&quot;) set_initial_value &lt;- function() { list( ) } ni &lt;- 10000 ; nt &lt;- 1 ; nb &lt;- 5000 ; nc &lt;- 3 model &lt;- jags(data, set_initial_value, parameters, model_string, n.chains = nc, n.thin = nt, n.iter = ni, n.burnin = nb) 3.4.2 Difference of Means This is a one-way anova with the intercept as the identity and the beta estimates representing the offset effects from the intercept. n &lt;- 1000 beta_0 &lt;- 1.5 beta_1 &lt;- 3 x &lt;- c(rep(0,n/2),rep(1,n/2)) mu &lt;- beta_0 + beta_1*x sd &lt;- 10 y &lt;- rnorm(n, mean = mu, sd = sd) #model.matrix(~x) lm_fit &lt;- lm(y ~ x) data &lt;- list( n = n, y = y, x = x ) model_string &lt;- textConnection( &quot; model { # Likelihood for (i in 1:n){ y[i] ~ dnorm(mu[i], tau) residuals[i] &lt;- y[i] - mu[i] mu[i] &lt;- intercept + beta_1 * x[i] } # Priors intercept ~ dnorm(0, 0.00001) beta_1 ~ dnorm(0, 0.00001) sigma ~ dunif(0, 100) # standard deviation tau &lt;- 1 / (sigma * sigma) # sigma^2 doesn&#39;t work in JAGS # Derived values residual_sum_sq &lt;- sum(residuals[]^2) } &quot; ) parameters &lt;- c(&quot;intercept&quot;, &quot;beta_1&quot;, &quot;sigma&quot;, &quot;residual_sum_sq&quot; ) set_initial_value &lt;- function() { list( ) } ni &lt;- 10000 ; nt &lt;- 1 ; nb &lt;- 5000 ; nc &lt;- 3 model &lt;- jags(data, set_initial_value, parameters, model_string, n.chains = nc, n.thin = nt, n.iter = ni, n.burnin = nb) 3.4.3 One-way ANOVA This is a one-way anova with the intercept as the identity and the beta estimates representing the offset effects from the intercept. n &lt;- 1000 beta_0 &lt;- 1.5 beta_1 &lt;- 3 beta_2 &lt;- -2 beta_3 &lt;- 0.2 x &lt;- as.factor(c(rep(1,n/4),rep(2,n/4),rep(3,n/4),rep(4,n/4))) X &lt;- model.matrix(~x) head(X) ## (Intercept) x2 x3 x4 ## 1 1 0 0 0 ## 2 1 0 0 0 ## 3 1 0 0 0 ## 4 1 0 0 0 ## 5 1 0 0 0 ## 6 1 0 0 0 mu &lt;- beta_0 + beta_1*X[,2] + beta_2*X[,3] + beta_3*X[,4] sd &lt;- 10 y &lt;- rnorm(n, mean = mu, sd = sd) lm_fit &lt;- lm(y ~ x) data &lt;- list( n = n, y = y, x = x ) model_string &lt;- textConnection( &quot; model { # Likelihood for (i in 1:n){ y[i] ~ dnorm(mu[i], tau) residuals[i] &lt;- y[i] - mu[i] mu[i] &lt;- intercept + betas[x[i]] } # Priors intercept ~ dnorm(0, 0.00001) betas[1] &lt;- 0 for(i in 2:4) { betas[i] ~ dnorm(0, 0.00001) } sigma ~ dunif(0, 100) # standard deviation tau &lt;- 1 / (sigma * sigma) # sigma^2 doesn&#39;t work in JAGS # Derived values residual_sum_sq &lt;- sum(residuals[]^2) } &quot; ) parameters &lt;- c(&quot;intercept&quot;, &quot;betas&quot;, &quot;sigma&quot;, &quot;residual_sum_sq&quot;) set_initial_value &lt;- function() { list( ) } ni &lt;- 10000 ; nt &lt;- 1 ; nb &lt;- 5000 ; nc &lt;- 3 model &lt;- jags(data, set_initial_value, parameters, model_string, n.chains = nc, n.thin = nt, n.iter = ni, n.burnin = nb) 3.4.4 One-way ANOVA Identity This is a one-way anova with an identity parameterization. Beta estimates represent the mean effect for each factor level. n &lt;- 1000 beta_0 &lt;- 3 beta_1 &lt;- 6 beta_2 &lt;- -4 beta_3 &lt;- 0 x &lt;- as.factor(c(rep(1,n/4),rep(2,n/4),rep(3,n/4),rep(4,n/4))) X &lt;- model.matrix(~x-1) tail(X) mu &lt;- beta_0*X[,1] + beta_1*X[,2] + beta_2*X[,3] + beta_3*X[,4] sd &lt;- 10 y &lt;- rnorm(n, mean = mu, sd = sd) lm_fit &lt;- lm(y ~ 0 + x) data &lt;- list( n = n, y = y, x = x ) model_string &lt;- textConnection( &quot; model { # Likelihood for (i in 1:n){ y[i] ~ dnorm(mu[i], tau) residuals[i] &lt;- y[i] - mu[i] mu[i] &lt;- betas[x[i]] } # Priors for(i in 1:4) { betas[i] ~ dnorm(0, 0.00001) } sigma ~ dunif(0, 100) # standard deviation tau &lt;- 1 / (sigma * sigma) # sigma^2 doesn&#39;t work in JAGS # Derived values residual_sum_sq &lt;- sum(residuals[]^2) } &quot; ) parameters &lt;- c(&quot;betas&quot;,&quot;sigma&quot;,&quot;residual_sum_sq&quot;) set_initial_value &lt;- function() { list( ) } ni &lt;- 10000 ; nt &lt;- 1 ; nb &lt;- 5000 ; nc &lt;- 3 model &lt;- jags(data, set_initial_value, parameters, model_string, n.chains = nc, n.thin = nt, n.iter = ni, n.burnin = nb) ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 1000 ## Unobserved stochastic nodes: 5 ## Total graph size: 3016 ## ## Initializing model ## ## | | | 0% | |++ | 4% | |++++ | 8% | |++++++ | 12% | |++++++++ | 16% | |++++++++++ | 20% | |++++++++++++ | 24% | |++++++++++++++ | 28% | |++++++++++++++++ | 32% | |++++++++++++++++++ | 36% | |++++++++++++++++++++ | 40% | |++++++++++++++++++++++ | 44% | |++++++++++++++++++++++++ | 48% | |++++++++++++++++++++++++++ | 52% | |++++++++++++++++++++++++++++ | 56% | |++++++++++++++++++++++++++++++ | 60% | |++++++++++++++++++++++++++++++++ | 64% | |++++++++++++++++++++++++++++++++++ | 68% | |++++++++++++++++++++++++++++++++++++ | 72% | |++++++++++++++++++++++++++++++++++++++ | 76% | |++++++++++++++++++++++++++++++++++++++++ | 80% | |++++++++++++++++++++++++++++++++++++++++++ | 84% | |++++++++++++++++++++++++++++++++++++++++++++ | 88% | |++++++++++++++++++++++++++++++++++++++++++++++ | 92% | |++++++++++++++++++++++++++++++++++++++++++++++++ | 96% | |++++++++++++++++++++++++++++++++++++++++++++++++++| 100% ## | | | 0% | |** | 4% | |**** | 8% | |****** | 12% | |******** | 16% | |********** | 20% | |************ | 24% | |************** | 28% | |**************** | 32% | |****************** | 36% | |******************** | 40% | |********************** | 44% | |************************ | 48% | |************************** | 52% | |**************************** | 56% | |****************************** | 60% | |******************************** | 64% | |********************************** | 68% | |************************************ | 72% | |************************************** | 76% | |**************************************** | 80% | |****************************************** | 84% | |******************************************** | 88% | |********************************************** | 92% | |************************************************ | 96% | |**************************************************| 100% 3.4.5 Two-way ANOVA n &lt;- 1000 beta_0 &lt;- 3 beta_1 &lt;- 6 beta_2 &lt;- -2 beta_3 &lt;- 3 x &lt;- as.factor(c(rep(1,n/2),rep(2,n/2))) x2 &lt;- as.factor(c(rep(1,n/4),rep(2,n/4),rep(1,n/4),rep(2,n/4))) X &lt;- model.matrix(~ x + x2 + x:x2) dim(X) ## [1] 1000 4 unique(X) ## (Intercept) x2 x22 x2:x22 ## 1 1 0 0 0 ## 251 1 0 1 0 ## 501 1 1 0 0 ## 751 1 1 1 1 mu &lt;- beta_0*X[,1] + beta_1*X[,2] + beta_2*X[,3] + beta_3*X[,4] sd &lt;- 3 y &lt;- rnorm(n, mean = mu, sd = sd) lm_fit &lt;- lm(y ~ x + x2 + x:x2) data &lt;- list( n = n, y = y, x = x, x2 = x2 ) model_string &lt;- textConnection( &quot; model { # Likelihood for (i in 1:n){ y[i] ~ dnorm(mu[i], tau) residuals[i] &lt;- y[i] - mu[i] mu[i] &lt;- intercept + betas[x[i],x2[i]] } # Priors intercept ~ dnorm(0, 0.00001) betas[1,1] &lt;- 0 betas[1,2] ~ dnorm(0, 0.00001) betas[2,1] ~ dnorm(0, 0.00001) betas[2,2] ~ dnorm(0, 0.00001) sigma ~ dunif(0, 100) # standard deviation tau &lt;- 1 / (sigma * sigma) # sigma^2 doesn&#39;t work in JAGS # Derived values residual_sum_sq &lt;- sum(residuals[]^2) } &quot; ) parameters &lt;- c(&quot;intercept&quot;,&quot;betas&quot;,&quot;sigma&quot;,&quot;residual_sum_sq&quot;) set_initial_value &lt;- function() { list( ) } ni &lt;- 10000 ; nt &lt;- 1 ; nb &lt;- 5000 ; nc &lt;- 3 model &lt;- jags(data, set_initial_value, parameters, model_string, n.chains = nc, n.thin = nt, n.iter = ni, n.burnin = nb) 3.4.6 ANCOVA n &lt;- 1000 beta_0 &lt;- 2 beta_1 &lt;- -2 beta_2 &lt;- 3 beta_3 &lt;- -1 x &lt;- as.factor(c(rep(1,n/2),rep(2,n/2))) year &lt;- round(runif(n,1,10)) X &lt;- model.matrix(~ x + year + x:year) tail(X) ## (Intercept) x2 year x2:year ## 995 1 1 3 3 ## 996 1 1 9 9 ## 997 1 1 4 4 ## 998 1 1 10 10 ## 999 1 1 8 8 ## 1000 1 1 5 5 unique(X) ## (Intercept) x2 year x2:year ## 1 1 0 8 0 ## 3 1 0 7 0 ## 4 1 0 5 0 ## 5 1 0 10 0 ## 6 1 0 3 0 ## 7 1 0 1 0 ## 8 1 0 4 0 ## 11 1 0 6 0 ## 13 1 0 9 0 ## 16 1 0 2 0 ## 501 1 1 9 9 ## 502 1 1 3 3 ## 503 1 1 7 7 ## 504 1 1 6 6 ## 505 1 1 2 2 ## 509 1 1 8 8 ## 511 1 1 4 4 ## 517 1 1 5 5 ## 525 1 1 10 10 ## 531 1 1 1 1 mu &lt;- beta_0*X[,1] + beta_1*X[,2] + beta_2*X[,3] + beta_3*X[,4] sd &lt;- 20 y &lt;- rnorm(n, mean = mu, sd = sd) #df &lt;- data.frame(y=y,x=x,year=year) #template.jags(y~ x + year + x:year, data = df) lm_fit &lt;- lm(y ~ x + year + x:year) data &lt;- list( n = n, y = y, x = x, year = year ) model_string &lt;- textConnection( &quot; model { # Likelihood for (i in 1:n){ y[i] ~ dnorm(mu[i], tau) residuals[i] &lt;- y[i] - mu[i] mu[i] &lt;- intercept + betas[x[i]] + year[i]*b_year[x[i]] } # Priors intercept ~ dnorm(0, 0.00001) betas[1] &lt;- 0 betas[2] ~ dnorm(0, 0.00001) b_year[1] ~ dnorm(0, 0.00001) b_year[2] ~ dnorm(0, 0.00001) sigma ~ dunif(0, 100) # standard deviation tau &lt;- 1 / (sigma * sigma) # sigma^2 doesn&#39;t work in JAGS # Derived values residual_sum_sq &lt;- sum(residuals[]^2) } &quot; ) parameters &lt;- c(&quot;intercept&quot;,&quot;betas&quot;,&quot;b_year&quot;,&quot;sigma&quot;,&quot;residual_sum_sq&quot;) set_initial_value &lt;- function() { list( ) } ni &lt;- 10000 ; nt &lt;- 1 ; nb &lt;- 5000 ; nc &lt;- 3 model &lt;- jags(data, set_initial_value, parameters, model_string, n.chains = nc, n.thin = nt, n.iter = ni, n.burnin = nb) 3.4.7 MANOVA 3.4.8 MANCOVA 3.5 Common Priors "],["abundance.html", "Chapter 4 Abundance 4.1 Lincoln-Peterson 4.2 Full Likelihood 4.3 Conditional Likelihood 4.4 Data Augmentation 4.5 N-Mixture Model 4.6 Distance Sampling 4.7 Spatial Capture-Recapture 4.8 Time to Event", " Chapter 4 Abundance 4.1 Lincoln-Peterson Lincoln 1930, Petersen 1896 The Lincoln-Petersen abundance estimator arises in cases where we have mark-recapture data recorded over two capture occasions. During the first occasion individuals are captured and uniquely marked. During the second occasion the number of individuals captured is recorded, along with the number of individuals marked during the first occasion that are recaptured. 4.1.1 Algebra The data consist of: \\(M_1\\) : the number of individuals captured and uniquely marked during the first occasion \\(M_2\\) : the number of previously marked individuals captured during the second occasion \\(C\\) : the number of animals captured during the second trapping occasion \\[M_2 \\sim Binomial(C, \\frac{M_1}{\\hat{N}}) \\] \\[\\mathcal{L}(C| M_1, p = \\frac{M_1}{\\hat{N}}) = {M_1 \\choose C} p^C(1-p)^{M_1-C}\\] The maximum likelihood estimate for \\(\\hat{N}\\) is, \\[\\hat{N} = \\frac{M_1*C}{M_2}\\] 4.1.2 Simulation R Python Julia set.seed(1) N &lt;- 75 # True population size n_occ &lt;- 2 # Number of trapping occasions p &lt;- 0.50 # Probability of first detection true_detections &lt;- array(NA, dim = c(N, n_occ)) for (t in 1:n_occ) { true_detections[, t] &lt;- rbinom(n = N, size = 1, prob = p) } observed &lt;- true_detections[apply(true_detections, 1, max) == 1, ] M_1 &lt;- sum(observed[, 1]) M_2 &lt;- nrow(subset(observed, rowSums(observed) &gt; 1)) C &lt;- sum(observed[, 2]) M_2 from pymc3 import * import pymc3 as pm import scipy as sp import numpy as np import arviz as az from IPython.display import display az.style.use(&quot;arviz-darkgrid&quot;) N = 75 n_occ = 2 p = .50 true_detections = sp.stats.bernoulli.rvs(p, size = (N, n_occ)) obs = true_detections[~np.all(true_detections == 0, axis=1)] M1, C = obs.sum(axis = 0) M2 = np.shape(obs[ obs.sum(axis=1) &gt; 1,])[0] using Distributions, MCMCChains, Random, StatsPlots, Turing Random.seed!(12) N = 150 n_occ = 4 p = .50 Y = Float64.(rand(length(p)) .&lt; p) 4.1.3 Models JAGS NIMBLE Stan Greta Pymc3 Pymc3 library(knitr) library(R2jags) data &lt;- list( M_1 = M_1, M_2= M_2, C = C, LB = C- M_1 + M_2, UB = 5*C ) model_string &lt;- textConnection( &quot; model { # Likelihood M_2 ~ dbin(M_1/N, C ) # Priors N ~ dunif(LB, UB) # Slightly informative prior # Derived values } &quot; ) parameters &lt;- c(&quot;N&quot;) set_initial_value &lt;- function() { list( ) } ni &lt;- 10000 ; nt &lt;- 1 ; nb &lt;- 5000 ; nc &lt;- 3 model &lt;- jags(data, set_initial_value, parameters, model_string, n.chains = nc, n.thin = nt, n.iter = ni, n.burnin = nb) library(nimble) n_data &lt;- list( M_1 = M_1, M_2= M_2, C = C ) n_constants &lt;- list( LB = C- M_1 + M_2, UB = 5*C ) Nimble_Code &lt;- nimbleCode({ # Likelihood M_2 ~ dbin(M_1/N, C ) # Priors N ~ dunif(LB, UB) # Slightly informative prior # Derived values }) n_params &lt;- c(&quot;N&quot;) n_inits &lt;- list( ) Nimble_Model &lt;- nimbleModel( code = Nimble_Code, constants = n_constants, data = n_data, inits = n_inits ) MCMC_Model &lt;- configureMCMC(Nimble_Model, monitors = n_params, print = T, enableWAIC = F) Model1_MCMC &lt;- buildMCMC(MCMC_Model) Comp_Model &lt;- compileNimble( Nimble_Model, showCompilerOutput = TRUE ) Comp_Model &lt;- compileNimble(Model1_MCMC, project = Nimble_Model) niter=10000 Model_samples &lt;- runMCMC(Comp_Model, niter = niter, nburnin=niter/2,nchains=3,summary = TRUE) # mcmc_combo(Model_samples$samples, pars = c(&quot;N&quot;, &quot;p&quot;)) # round(Model_samples$summary$all.chains,2) H library(knitr) library(rstan) data &lt;- list( M_1 = M_1, M_2= M_2, C = C ) stan_model &lt;- &quot; data { int&lt;lower=0&gt; M_1; int&lt;lower=0&gt; M_2; int&lt;lower=0&gt; C; } parameters { real&lt;lower=(C - M_2 + M_1)&gt; N; } model { M_2 ~ binomial(C, M_1 / N); } &quot; nc &lt;- 4 stan.samples &lt;- stan(model_code = stan_model, data = data, iter = 10000, chains = nc, cores = nc) library(greta) LB &lt;- C- M_1 + M_2 UB &lt;- 5*C # priors N &lt;- uniform(LB,UB) # likelihood distribution(M_2) &lt;- binomial(C, M_1 / N) # derived parameter # defining the model m &lt;- model(N) #objects to sample # sampling draws &lt;- greta::mcmc(m, n_samples = 1000) LB = C - M1 + M2 UB = 5*C trials = 1 LB with Model() as model: # Priors for unknown model parameters N_prior = pm.Uniform(name = &quot;N&quot;, lower = LB, upper = UB) # Likelihood (sampling distribution) of observations N = pm.Binomial(name = &quot;N_hat&quot;, p = M1/N_prior, observed = M2, n = C) trace = pm.sample(); with model: display(az.summary(trace, round_to = 2)); with model: display(az.summary(trace, round_to = 2)); with Model() as model: # model specifications in PyMC3 are wrapped in a with-statement # Priors for unknown model parameters p = pm.Uniform(&#39;p&#39;, 0., 1.) # Likelihood (sampling distribution) of observations pstar = 1-(1-p)*(1-p)*(1-p)*(1-p) N_hat = pm.Deterministic(&quot;N_hat&quot;, MNKA / pstar) Y_obs = pm.Bernoulli(&quot;Y_obs&quot;, p = p, observed=Y) trace = pm.sample() with model: display(az.summary(trace, round_to = 2)) 4.1.4 Comparison 4.2 Full Likelihood 4.3 Conditional Likelihood The conditional likelihood abundance estimator proposed by Huggins (1989) and Alho (1990), which was further extended in Huggins (1991), is an extension to previous abundance estimators to account for heterogeneous capture probabilities (\\(p\\)). The model estimates individual capture probabilities and abundance conditional on captured individuals. 4.3.1 Algebra The capture history \\(y_{i,t}\\) is used to estimate the capture probability of individual \\(i\\) as a Bernoulli trial, \\[y_{i,t} \\sim Bernoulli(p_{i,t})\\] \\[\\mathcal{L}(p| y) = \\prod_{i=1}^n \\prod_{t=1}^t p_{i,t}^{z_{i,t}}(1-p_{i,t})^{1-z_{i,t}}\\] Abundance \\(\\hat{N}\\) is derived conditional on the count of known individuals (\\(C\\)), sometimes referred to the minimum number of known alive (\\(MNKA\\)). \\[\\hat{N} = \\frac{C}{1-\\prod^{t}(1-p_t)}\\] Variation in detection probability can be modeled using linear logistic models or other variations used to estimate probabilities 0-1. 4.3.2 Simulation R Python Julia set.seed(1) N &lt;- 150 #True population size n_occ &lt;- 4 #Number of trapping occasions p &lt;- 0.50 #Probability of first detection true_detections &lt;- array(NA, dim=c(N,n_occ)) for (t in 1:n_occ){ true_detections[,t] &lt;- rbinom(n=N,size=1,prob=p) } observed &lt;- true_detections[apply(true_detections,1,max) == 1,] MNKA &lt;- nrow(observed) print( paste0(&quot;Number ever detected: &quot;, MNKA,sep = &quot; &quot;) ) #number ever detected from pymc3 import * import scipy as sp import numpy as np import arviz as az from IPython.display import display az.style.use(&quot;arviz-darkgrid&quot;) N = 150 n_occ = 4 p = .50 Y = sp.stats.bernoulli.rvs(p, size = (N, n_occ)) obs = Y[~np.all(Y == 0, axis=1)] MNKA = len(obs) 4.3.3 Models JAGS NIMBLE Stan Greta Pymc3 Pymc3 library(R2jags) data &lt;- list( y=observed, n_sites=nrow(observed), MNKA=MNKA, n_occ=n_occ ) model_string &lt;- textConnection( &quot; model { # Likelihood for(i in 1:n_sites) { # Observation model for(j in 1:n_occ) { y[i, j] ~ dbern(p) } } for(t in 1:n_occ){ p_un[t] &lt;- (1-p) } # Priors p ~ dunif(0, 1) # Uninformative prior # Derived values N &lt;- (MNKA / (1-prod(p_un[]))) }&quot;) parameters &lt;- c(&quot;p&quot;,&quot;N&quot;) inits &lt;- function() { list( ) } ni &lt;- 10000 ; nt &lt;- 1 ; nc &lt;- 3 model &lt;- jags(data = data, inits = inits, parameters = parameters, model.file = model_string, n.chains = nc, n.thin = nt, n.iter = ni, n.burnin = ni/2) library(nimble) n_data &lt;- list( y = observed, MNKA = MNKA ) n_constants &lt;- list( n_occ = n_occ ) Nimble_Code &lt;- nimbleCode({ # Likelihood for(i in 1:MNKA) { # Observation model for(j in 1:n_occ) { y[i, j] ~ dbern(p) } #j } #i # Priors p ~ dunif(0, 1) # Uninformative prior # Derived values for(t in 1:n_occ){ p_un[t] &lt;- (1-p) } #t N &lt;- (MNKA / (1-prod(p_un[1:n_occ]))) #The only difference in this model is here declaring dimensions }) n_params &lt;- c(&quot;p&quot;, &quot;N&quot;) n_inits &lt;- list( ) Nimble_Model &lt;- nimbleModel( code = Nimble_Code, constants = n_constants, data = n_data, inits = n_inits ) MCMC_Model &lt;- configureMCMC(Nimble_Model, monitors = n_params, print = T, enableWAIC = F) Model1_MCMC &lt;- buildMCMC(MCMC_Model) Comp_Model &lt;- compileNimble( Nimble_Model, showCompilerOutput = TRUE ) Comp_Model &lt;- compileNimble(Model1_MCMC, project = Nimble_Model) ni &lt;- 10000 ; nt &lt;- 1 ; nc &lt;- 3 Model_samples &lt;- runMCMC(mcmc = Comp_Model, niter = ni, nburnin = ni/2, nchains = nc, summary = TRUE) #mcmc_combo(Model_samples$samples, pars = c(&quot;N&quot;, &quot;p&quot;)) round(Model_samples$summary$all.chains,2) library(rstan) data &lt;- list( y=observed, nsites=nrow(observed), MNKA=MNKA, n_occ=n_occ ) stan_model &lt;- &quot; data { int&lt;lower=0&gt; MNKA; int&lt;lower=0&gt; nsites; int&lt;lower=0&gt; n_occ; int&lt;lower=0,upper=1&gt; y[MNKA, n_occ]; } parameters { real&lt;lower=0, upper=1&gt; p; } model { for(i in 1:nsites) for(j in 1:4) target += bernoulli_lpmf(y[i, j] | p); } generated quantities { real pstar = (1-(1-p)^n_occ); real N = MNKA / pstar; } &quot; ni &lt;- 10000 ; nt &lt;- 1 ; nc &lt;- 4 stan.samples &lt;- stan(model_code = stan_model, data = data, iter = ni, warmup = floor(ni/2), chains = nc, cores = nc) library(reticulate) reticulate::use_condaenv(&quot;r-reticulate&quot;) reticulate::py_config() library(greta) capture_vec &lt;- unlist(observed) # priors p_greta &lt;- beta(1, 1) # likelihood distribution(capture_vec) &lt;- bernoulli(p_greta) # derived parameters pstar &lt;- 1 - (1 - p_greta)^n_occ N_hat &lt;- MNKA / pstar # defining the model m &lt;- model(p_greta, N_hat, pstar) # sampling ni &lt;- 10000 ; nt &lt;- 1 ; nc &lt;- 4 draws &lt;- greta::mcmc(model = m, n_samples = ni, thin = nt, warmup = floor(ni/2), chains = nc, n_cores = nc) with Model() as model: # model specifications in PyMC3 are wrapped in a with-statement # Priors for unknown model parameters p = pm.Uniform(&#39;p&#39;, 0., 1.) # Likelihood (sampling distribution) of observations pstar = 1-(1-p)*(1-p)*(1-p)*(1-p) N_hat = pm.Deterministic(&quot;N_hat&quot;, MNKA / pstar) Y_obs = pm.Bernoulli(&quot;Y_obs&quot;, p = p, observed=Y) trace = pm.sample() with model: display(az.summary(trace, round_to = 2)) 4.3.4 Comparison 4.4 Data Augmentation 4.5 N-Mixture Model 4.6 Distance Sampling 4.7 Spatial Capture-Recapture 4.8 Time to Event References "],["occupancy.html", "Chapter 5 Occupancy 5.1 Static Single Season 5.2 Multistate 5.3 Multiscale 5.4 Multiscale Multistate 5.5 Species Cooccurrence 5.6 Dynamic Occupancy", " Chapter 5 Occupancy We describe our methods in this chapter. 5.1 Static Single Season 5.2 Multistate 5.3 Multiscale 5.4 Multiscale Multistate 5.5 Species Cooccurrence 5.6 Dynamic Occupancy "],["survival.html", "Chapter 6 Survival 6.1 Nest Survival 6.2 Known-fate 6.3 Individual-level Cormack-Jolly-Seber 6.4 Jolly-Seber", " Chapter 6 Survival 6.1 Nest Survival 6.2 Known-fate 6.3 Individual-level Cormack-Jolly-Seber Model description 6.3.1 Algebra Log likelihood of the state-space parametrization \\[\\mathcal{L}(\\phi, p, z| y) = f(z_1|\\phi) \\prod_{t=2}^T f(z_t| z_{t-1}, \\phi) \\prod_{t=1}^T f(y_t| z_t, p)\\] State Process \\[z_{i_f} = 1\\] \\[z_{i,t+1}|z_{i,t} \\sim Bernoulli(z_{i,t} \\phi_{i,t}) \\] Observation Process \\[y_{i,t}|z_{i,t} \\sim Bernoulli(z_{i,t},p_{i,t})\\] 6.3.2 Simulation R Python Julia n_occ &lt;- 4 # Number of capture occasions marked &lt;- rep(50, n_occ-1) # Annual number of newly marked individuals phi &lt;- rep(0.65, n_occ-1) p &lt;- rep(0.4, n_occ-1) # Define matrices with survival and recapture probabilities PHI &lt;- matrix(phi, ncol = n_occ-1, nrow = sum(marked)) P &lt;- matrix(p, ncol = n_occ-1, nrow = sum(marked)) CH &lt;- matrix(0, ncol = n_occ, nrow = sum(marked)) marking_occ &lt;- rep(1:length(marked), marked[1:length(marked)]) # Fill the CH matrix i&lt;-1 for (i in 1:sum(marked)){ CH[i, marking_occ[i]] &lt;- 1 # Write an 1 at the release occasion if (marking_occ[i]==n_occ) next for (t in (marking_occ[i]+1):n_occ){ survive_occasion &lt;- rbinom(1, 1, PHI[i,t-1]) if (survive_occasion==0) break rp &lt;- rbinom(1, 1, P[i,t-1]) if (rp==1) CH[i,t] &lt;- 1 } #t } #i get.first.capture &lt;- function(x) min(which(x!=0)) first_capture &lt;- apply(CH, 1, get.first.capture) get.last.capture &lt;- function(x) max(which(x!=0)) last_capture &lt;- apply(CH, 1, get.last.capture) 6.3.3 Models JAGS NIMBLE Stan Greta library(knitr) library(R2jags) data &lt;- list( y = CH, n_ind = dim(CH)[1], nocc = dim(CH)[2], f = first_capture ) model_string &lt;- textConnection( &quot; model { # Likelihood for (i in 1:n_ind){ z[i,f[i]] &lt;- 1 for (t in (f[i]+1):n_occ){ # State process z[i,t] ~ dbern(phi * z[i,t-1]) # Observation process y[i,t] ~ dbern(mu[i,t]) mu[i,t] &lt;- p * z[i,t] } #t } #i # Priors and Constraints phi ~ dunif(0, 1) p ~ dunif(0, 1) # Derived values } &quot; ) parameters &lt;- c(&quot;p&quot;,&quot;phi&quot;) cjs.z.init &lt;- function(ch){ state &lt;- ch for (i in 1:dim(ch)[1]){ n1 &lt;- min(which(ch[i,]==1)) n2 &lt;- max(which(ch[i,]==1)) state[i,n1:n2] &lt;- 1 state[i,n1] &lt;- NA } state[state==0] &lt;- NA return(state) } set_initial_value &lt;- function() { list( z = cjs.z.init(CH) ) } ni &lt;- 10000 ; nt &lt;- 1 ; nc &lt;- 3 model &lt;- jags(data = data, inits = inits, parameters = parameters, model.file = model_string, n.chains = nc, n.thin = nt, n.iter = ni, n.burnin = ni/2) library(nimble) n_data &lt;- list( y = CH ) n_constants &lt;- list( n_ind = dim(CH)[1], nocc = dim(CH)[2], f = first_capture ) Nimble_Code &lt;- nimbleCode({ # Likelihood for (i in 1:n_ind){ z[i,f[i]] &lt;- 1 for (t in (f[i]+1):n_occ){ # State process z[i,t] ~ dbern(phi * z[i,t-1]) # Observation process y[i,t] ~ dbern(mu[i,t]) mu[i,t] &lt;- p * z[i,t] } #t } #i # Priors and Constraints phi ~ dunif(0, 1) p ~ dunif(0, 1) # Derived values }) n_params &lt;- c(&quot;p&quot;, &quot;phi&quot;) cjs.z.init &lt;- function(ch){ state &lt;- ch for (i in 1:dim(ch)[1]){ n1 &lt;- min(which(ch[i,]==1)) n2 &lt;- max(which(ch[i,]==1)) state[i,n1:n2] &lt;- 1 state[i,n1] &lt;- NA } state[state==0] &lt;- NA return(state) } n_inits &lt;- list( z = cjs.z.init(CH) ) Nimble_Model &lt;- nimbleModel( code = Nimble_Code, constants = n_constants, data = n_data, inits = n_inits ) MCMC_Model &lt;- configureMCMC(Nimble_Model, monitors = n_params, print = T, enableWAIC = F) Model1_MCMC &lt;- buildMCMC(MCMC_Model) Comp_Model &lt;- compileNimble( Nimble_Model, showCompilerOutput = TRUE ) Comp_Model &lt;- compileNimble(Model1_MCMC, project = Nimble_Model) ni &lt;- 10000 ; nt &lt;- 1 ; nc &lt;- 3 Model_samples &lt;- runMCMC(mcmc = Comp_Model, niter = ni, nburnin = ni/2, nchains = nc, summary = TRUE) #mcmc_combo(Model_samples$samples, pars = c(&quot;N&quot;, &quot;p&quot;)) round(Model_samples$summary$all.chains,2) library(knitr) library(rstan) data &lt;- list( CH = CH, n_ind = dim(CH)[1], n_occ = dim(CH)[2], f = first_capture, l = last_capture ) stan_model &lt;- &quot; /** * Cormack-Jolly-Seber Model * * following section 1.2.1 of: * http://www.maths.otago.ac.nz/home/resources/theses/PhD_Matthew_Schofield.pdf * https://discourse.mc-stan.org/t/cjs-log-likelihood/15112 */ data { int&lt;lower=2&gt; n_occ; // capture events int&lt;lower=0&gt; n_ind; // number of individuals int&lt;lower=0, upper=n_occ+1&gt; f[n_ind]; // f[i]: ind i first capture int&lt;lower=0, upper=n_occ+1&gt; l[n_ind]; // l[i]: ind i last capture int&lt;lower=0,upper=1&gt; CH[n_ind,n_occ]; // CH[i,k]: individual i captured at k } transformed data { int&lt;lower=0,upper=n_ind&gt; n_captured[n_occ]; // n_capt[k]: num aptured at k n_captured = rep_array(0,n_occ); for (i in 1:n_ind) for (k in 1:n_occ) n_captured[k] = n_captured[k] + CH[i,k]; } parameters { //vector&lt;lower=0,upper=1&gt;[n_occ-1] phi; // phi[k]: Pr[alive at k + 1 | alive at k] //vector&lt;lower=0,upper=1&gt;[n_occ] p; // p[k]: Pr[capture at k] real&lt;lower=0,upper=1&gt; phi; // phi[k]: Pr[alive at k + 1 | alive at k] real&lt;lower=0,upper=1&gt; p; // p[k]: Pr[capture at k] // note: p[1] not used in model and hence not identified } transformed parameters { vector&lt;lower=0,upper=1&gt;[n_occ] chi; // chi[k]: Pr[no capture &gt; k | alive at k] vector[n_ind] log_lik; { int k; chi[n_occ] = 1.0; k = n_occ - 1; while (k &gt; 0) { //chi[k] = (1 - phi[k]) + phi[k] * (1 - p[k+1]) * chi[k+1]; chi[k] = (1 - phi) + phi * (1 - p) * chi[k+1]; k = k - 1; } } for (i in 1:n_ind) { log_lik[i] = 0; if (l[i] &gt; 0) { for (k in (f[i]+1):l[i]) { log_lik[i] +=log(phi); // i survived from k-1 to k if (CH[i,k] == 1) log_lik[i] +=log(p); // i captured at k else log_lik[i] +=log1m(p); // i not captured at k } log_lik[i] +=log(chi[l[i]]); // i not seen after last[i] } } } model { target += sum(log_lik); } generated quantities { // phi[K-1] and p(K) not identified, but product is real beta; vector&lt;lower=0&gt;[n_occ] pop_hat; // population beta = phi * p; for (k in 1:n_occ) pop_hat[k] = n_captured[k] / p; } &quot; inits &lt;- function() list(phi = runif(1, 0, 1), p = runif(1, 0, 1)) ## Parameters monitored params &lt;- c(&quot;phi&quot;, &quot;p&quot;) ## MCMC settings ni &lt;- 10000 ; nt &lt;- 1 ; nc &lt;- 4 stan.samples &lt;- stan(model_code = stan_model, data = data, iter = ni, warmup = floor(ni/2), chains = nc, cores = nc) library(greta) head(CH) head(obs_id) head(capture_vec) obs_id &lt;- apply(CH, 1, function(x) seq(min(which(x &gt; 0)), max(which(x &gt; 0)), by = 1)[-1]) obs_id &lt;- unlist(obs_id) capture_vec &lt;- apply(CH, 1, function(x) x[min(which(x &gt; 0)):max(which(x &gt; 0))][-1]) capture_vec &lt;- unlist(capture_vec) # dummy variables alive_data &lt;- ones(length(obs_id)) # definitely alive not_seen_last &lt;- last_capture != n_occ # ignore observations in last timestep final_observation &lt;- ones(sum(not_seen_last)) # final observation capture_vec &lt;- as_data(observed) # priors phi &lt;- beta(1, 1, dim = 1) p &lt;- beta(1, 1, dim = 1) # derived parameter chi &lt;- ones(n_occ) for (i in seq_len(n_occ - 1)) { tn &lt;- n_occ - i chi[tn] &lt;- (1 - phi) + phi * (1 - p) * chi[tn + 1] } # likelihood distribution(alive_data) &lt;- bernoulli(phi) distribution(capture_vec) &lt;- bernoulli(p) distribution(final_observation) &lt;- bernoulli(chi[last_capture[not_seen_last]]) # defining the model m &lt;- model(phi,p) #objects to sample # sampling ni &lt;- 10000 ; nt &lt;- 1 ; nc &lt;- 4 draws &lt;- greta::mcmc(model = m, n_samples = ni, thin = nt, warmup = floor(ni/2), chains = nc, n_cores = nc) 6.3.4 Comparison ## ── Attaching packages ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse 1.3.1 ── ## ✓ ggplot2 3.3.5 ✓ purrr 0.3.4 ## ✓ tibble 3.1.2 ✓ dplyr 1.0.7 ## ✓ tidyr 1.1.3 ✓ stringr 1.4.0 ## ✓ readr 1.4.0 ✓ forcats 0.5.1 ## ── Conflicts ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ── ## x tidyr::expand() masks Matrix::expand() ## x tidyr::extract() masks runjags::extract() ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() ## x tidyr::pack() masks Matrix::pack() ## x tidyr::unpack() masks Matrix::unpack() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #altgbyfeur .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #altgbyfeur .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #altgbyfeur .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #altgbyfeur .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; border-top-color: #FFFFFF; border-top-width: 0; } #altgbyfeur .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #altgbyfeur .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #altgbyfeur .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #altgbyfeur .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #altgbyfeur .gt_column_spanner_outer:first-child { padding-left: 0; } #altgbyfeur .gt_column_spanner_outer:last-child { padding-right: 0; } #altgbyfeur .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #altgbyfeur .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #altgbyfeur .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #altgbyfeur .gt_from_md > :first-child { margin-top: 0; } #altgbyfeur .gt_from_md > :last-child { margin-bottom: 0; } #altgbyfeur .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #altgbyfeur .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #altgbyfeur .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #altgbyfeur .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #altgbyfeur .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #altgbyfeur .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #altgbyfeur .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #altgbyfeur .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #altgbyfeur .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #altgbyfeur .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #altgbyfeur .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #altgbyfeur .gt_sourcenote { font-size: 90%; padding: 4px; } #altgbyfeur .gt_left { text-align: left; } #altgbyfeur .gt_center { text-align: center; } #altgbyfeur .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #altgbyfeur .gt_font_normal { font-weight: normal; } #altgbyfeur .gt_font_bold { font-weight: bold; } #altgbyfeur .gt_font_italic { font-style: italic; } #altgbyfeur .gt_super { font-size: 65%; } #altgbyfeur .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 65%; } Estimate Comparison Comparison of C-J-S apparent survival (phi) and recapture probability (p) estimates fit using JAGS, Stan, greta. Mean SD LCL UCL phi 0.65 NA NA NA 0.60 0.08 0.47 0.78 0.61 0.08 0.46 0.78 0.62 0.07 0.49 0.78 p 0.40 NA NA NA 0.44 0.08 0.29 0.60 0.43 0.08 0.28 0.60 0.46 0.07 0.32 0.61 6.4 Jolly-Seber "],["appendix-a.html", "A Appendix A A.1 Authors Guidelines", " A Appendix A A.1 Authors Guidelines We have decided to adapt the tidyverse style guide. Detailed content can be found here. tidyverse style guide Before committing code to the git repository, it should be styled using the styler package. Primary points are outlined below: - Use underscores (snake_case) to separate words in both variable and file names. - Use periods to separate words in function names File Names: chapter_1.Rmd --- Variable Names: observed_data When submitting a description of a new model use the template provided in Appendix B. However, if extenuating circumstances make the template in Appendix B untenable for a model, describe the reason for departure in the git commit message. Variables should be named with nouns, functions should be named with verbs Use dots to separate words in function names simulate.code &lt;- function(X) { } Label function arguments Spaces after commas not before Do not pad parentheses with spaces Pad operators with space foo == bar foo &lt;- bar foo + bar foo * bar "],["appendix-b.html", "B Appendix B B.1 Model Template", " B Appendix B B.1 Model Template ## Model Name Model description ### Algebra Algebraic model description in latex ### Simulation {.tabset} ::: {.tab} &lt;button class=&quot;tablinks&quot; onclick=&quot;unrolltab(event,&#39;R_XXX_sim&#39;)&quot;&gt;R&lt;/button&gt; &lt;button class=&quot;tablinks&quot; onclick=&quot;unrolltab(event,&#39;Python_XXX_sim&#39;)&quot;&gt;Python&lt;/button&gt; &lt;button class=&quot;tablinks&quot; onclick=&quot;unrolltab(event,&#39;Julia_XXX_sim&#39;)&quot;&gt;Julia&lt;/button&gt; ::: {#R_XXX_sim .tabcontent} #### {-} &lt;br&gt; ```{r , eval = FALSE}r &#39;&#39;` ``` ::: ::: {#Python_XXX_sim .tabcontent} #### {-} &lt;br&gt; ```{python , eval = FALSE}r &#39;&#39;` ``` ::: ::: {#Julia_XXX_sim .tabcontent} #### {-} &lt;br&gt; ```{julia , eval = FALSE}r &#39;&#39;` ``` ::: ::: ### Models {.tabset} ::: {.tab} &lt;button class=&quot;tablinks&quot; onclick=&quot;unrolltab(event,&#39;JAGS_XXX_mod&#39;)&quot;&gt;JAGS&lt;/button&gt; &lt;button class=&quot;tablinks&quot; onclick=&quot;unrolltab(event,&#39;NIMBLE_XXX_mod&#39;)&quot;&gt;NIMBLE&lt;/button&gt; &lt;button class=&quot;tablinks&quot; onclick=&quot;unrolltab(event, &#39;Stan_XXX_mod&#39;)&quot;&gt;Stan&lt;/button&gt; &lt;button class=&quot;tablinks&quot; onclick=&quot;unrolltab(event, &#39;Greta_XXX_mod&#39;)&quot;&gt;Greta&lt;/button&gt; &lt;button class=&quot;tablinks&quot; onclick=&quot;unrolltab(event,&#39;Pymc3_XXX_mod&#39;)&quot;&gt;Pymc3&lt;/button&gt; &lt;button class=&quot;tablinks&quot; onclick=&quot;unrolltab(event,&#39;Turing_XXX_mod&#39;)&quot;&gt;Pymc3&lt;/button&gt; ::: {#JAGS_XXX_mod .tabcontent} #### {-} &lt;br&gt; ```{r ,eval = FALSE}r &#39;&#39;` library(knitr) library(R2jags) data &lt;- list( ) model_string &lt;- textConnection( &quot; model { # Likelihood # Priors # Derived values } &quot; ) parameters &lt;- c( ) set_initial_value &lt;- function() { list( ) } ni &lt;- 10000 ; nt &lt;- 1 ; nc &lt;- 3 model &lt;- jags(data = data, inits = inits, parameters = parameters, model.file = model_string, n.chains = nc, n.thin = nt, n.iter = ni, n.burnin = ni/2) ``` ::: ::: {#NIMBLE_XXX_mod .tabcontent} #### {-} &lt;br&gt; ```{r ,eval = FALSE}r &#39;&#39;` library(nimble) n_data &lt;- list( ) n_constants &lt;- list( ) Nimble_Code &lt;- nimbleCode({ # Likelihood # Priors # Derived values }) n_params &lt;- c( ) n_inits &lt;- list( ) Nimble_Model &lt;- nimbleModel( code = Nimble_Code, constants = n_constants, data = n_data, inits = n_inits ) MCMC_Model &lt;- configureMCMC(Nimble_Model, monitors = n_params, print = T, enableWAIC = F) Model1_MCMC &lt;- buildMCMC(MCMC_Model) Comp_Model &lt;- compileNimble( Nimble_Model, showCompilerOutput = TRUE ) Comp_Model &lt;- compileNimble(Model1_MCMC, project = Nimble_Model) ni &lt;- 10000 ; nt &lt;- 1 ; nc &lt;- 3 Model_samples &lt;- runMCMC(mcmc = Comp_Model, niter = ni, nburnin = ni/2, nchains = nc, summary = TRUE) mcmc_combo(Model_samples$samples, pars = c(&quot;N&quot;, &quot;p&quot;)) round(Model_samples$summary$all.chains,2) ``` ::: ::: {#Stan_XXX_mod .tabcontent} #### {-} &lt;br&gt; ```{r ,eval = FALSE}r &#39;&#39;` library(knitr) library(rstan) data &lt;- list( ) stan_model &lt;- &quot; data { } parameters { } model { } generated quantities { } &quot; ni &lt;- 10000 ; nt &lt;- 1 ; nc &lt;- 4 stan.samples &lt;- stan(model_code = stan_model, data = data, iter = ni, warmup = floor(ni/2), chains = nc, cores = nc) ``` ::: ::: {#Greta_XXX_mod .tabcontent} #### {-} &lt;br&gt; ```{r ,eval = FALSE}r &#39;&#39;` library(greta) # priors # likelihood # derived parameter # defining the model m &lt;- model() #objects to sample # sampling ni &lt;- 10000 ; nt &lt;- 1 ; nc &lt;- 4 draws &lt;- greta::mcmc(model = m, n_samples = ni, thin = nt, warmup = floor(ni/2), chains = nc, n_cores = nc) ``` ::: ::: {#Pymc3_XXX_mod .tabcontent} #### {-} &lt;br&gt; ```{python ,eval = FALSE}r &#39;&#39;` with Model() as model: # Priors for unknown model parameters # Likelihood (sampling distribution) of observations trace = pm.sample() ``` ::: ::: {#Turing_XXX_mod .tabcontent} #### {-} &lt;br&gt; ```{julia ,eval = FALSE}r &#39;&#39;` using Turing, Distributions @model function linear_regression(x, y) end model = linear_regression(x, y) chain = sample(model, NUTS(0.65), 3_000); ``` ::: ::: ### Comparison ```{r , echo=FALSE, warning=FALSE}r``` #round(model$BUGSoutput$summary[c(1,3),c(1,2,3,7)],2) #jags #round(summary(stan.samples)$summary[c(2,1),c(1,3,4,8)],2) #stan #summary(draws) #greta #https://gt.rstudio.com/articles/intro-creating-gt-tables.html library(gt) library(tidyverse) results_df &lt;- data.frame( Sampler=c(), Parameter=c(), Mean=c(), SD=c(), LCL=c(), UCL=c() ) results_df %&gt;% dplyr::select(-Sampler,-Parameter) %&gt;% gt() %&gt;% tab_row_group( group = &quot;p&quot;, rows = 5:8 ) %&gt;% tab_row_group( group = &quot;N&quot;, rows = 1:4 ) %&gt;% tab_header( title = &quot;&quot;, subtitle = &quot;&quot; ) ``` "],["references.html", "References", " References "]]
